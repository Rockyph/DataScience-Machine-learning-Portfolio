{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c00696c-e6fe-4483-868e-3865f71812b0",
   "metadata": {},
   "source": [
    "# Multilevel Perceptron implementaton and walktrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202d753e-a35b-4089-8815-d184daa4734d",
   "metadata": {},
   "source": [
    "Welcome! In this notebook, I will be implementing a Fully connected neural network from scratch. The goal of this notebook is to portray my understanding of the underlying mechanics involved in training a neural network. The code is structured in a \"bottom up\" way, so after successfully implementing an element from scratch like a linear layer, I will be further substiuting it with the Pytorch equivelent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d328f1-288c-4509-ae76-c8c5d47dd04c",
   "metadata": {},
   "source": [
    "So, lets not waste any more time and dive straight into it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a23cc-e14c-46b4-bcc5-6d9ff26f56eb",
   "metadata": {},
   "source": [
    "Lets begin with some standard imports like matplotlib for the visualization of our data. For this walktrough, we will be working with the classic MNIST Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acadf4fe-7d45-4cf6-bef2-93281d3ce5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## necessary imports\n",
    "import pickle,gzip,math,os,time,shutil,torch\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f153b4b-d095-467c-9621-b2ae37ef30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import data\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: \n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])\n",
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e6e88-a791-422f-b1d3-ac5e7a2950aa",
   "metadata": {},
   "source": [
    "Lets take a look at the first image from our training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d590ede9-0af3-44a6-9e2a-597a67147edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12f9b2c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67655807-602d-4f78-ab92-e0b4ae94311d",
   "metadata": {},
   "source": [
    "Lets take a look of the shapes of the data and the amount of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2624b8-b353-4d89-b4ee-7a1b88713b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "classes = y_train.max()+1\n",
    "n, m, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd37b8-771b-4f47-ab69-940718be7bb8",
   "metadata": {},
   "source": [
    "# Starting with a very basic architecture - Foundation version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234e03d-2904-4856-b438-0f51d6ca43a8",
   "metadata": {},
   "source": [
    "Lets start with initializing the weights and biases for a simple two layer architecture. The num_hidden variable represents the number of neurons (units) in the hidden layer of the neural network. I decided to move forward with 50 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044105b-cc6f-4c58-8007-0865879a70d4",
   "metadata": {},
   "source": [
    "These parameters (initialized as random value) will be updated in the training process when the network will be learning from the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8950810-de90-4e55-bc08-3ec1cc5c0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "num_hidden = 50 \n",
    "\n",
    "w1 = torch.randn(m,num_hidden)\n",
    "b1 = torch.zeros(num_hidden)\n",
    "w2 = torch.randn(num_hidden,1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5e26a-c9e7-457e-a340-4c9dc845103d",
   "metadata": {},
   "source": [
    "The Module class below is a simplified imitation of the 'nn.Module' class in PyTorch. The purpose of this class is that it represents a generic neural network module and provides a structure for building custom modules. The __call__ method is invoked when an instance of the class is called as a function. It later returns the output by calling the forward method with the appropriate arguments.\n",
    "\n",
    "Both the forward and bwd methods are supposed to be implemented in the subclasses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34011c52-1033-469a-ab48-cd6a6b37e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): \n",
    "        raise Exception('not implemented')\n",
    "        \n",
    "    def backward(self): \n",
    "        self.bwd(self.out, *self.args)\n",
    "        \n",
    "    def bwd(self): \n",
    "        raise Exception('not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c690e3b-4a5f-4ad3-bf6f-4561a8b014b8",
   "metadata": {},
   "source": [
    "It is important to include non Linearity in our model. A simple way of achieving that is to implement the Rectified Linear Unit (ReLU). It works by allowing positive values to pass through unchanged, while setting all negative values to zero. Lets see how that would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8530aea2-9ffb-41ac-b8ae-8fd21388df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): \n",
    "        return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): # Implementing the bwd method to compute the gradients and updates the inp gradients \n",
    "        inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c530758-e7b3-48d7-80c6-59e742541a8c",
   "metadata": {},
   "source": [
    "The code below defines a linear layer in a neural network. This class extends the previously defined Module class and includes methods for the forward pass and backward pass.\n",
    "- the forward method computes the linear transformation by multiplying the input tensor by the weight matrix (inp @ self.w) and adding the bias vector (+ self.b).\n",
    "- The bwd method is responsible for the backward pass, calculating gradients with respect to the input (inp), weight matrix (self.w), and bias vector (self.b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d048fe-26f8-4444-81f5-4a9c1977dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our Linear layer class, with initialized weights and biases and backward pass\n",
    "class Linear(Module):\n",
    "    def __init__(self, w, b): \n",
    "        self.w,self.b = w,b\n",
    "    \n",
    "    def forward(self, inp): \n",
    "        return inp@self.w + self.b\n",
    "    \n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84f01bb-6948-4b01-bce4-60268f29b3c6",
   "metadata": {},
   "source": [
    "Below is our first provisional loss function Mse (Mean Squared Error). This loss function is a starting point, and will be substituted with the more common Cross-Entropy Loss for classification tasks later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ed6526-b918-467b-b630-dac08f1b8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward (self, inp, targ): \n",
    "        return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ): \n",
    "        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b317fae-56e0-41d0-bf62-4730ed7198bc",
   "metadata": {},
   "source": [
    "*Constructing our model class*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38bfd09-472a-4d0d-9932-2765a108f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Linear(w1,b1), Relu(), Linear(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        self.preds = x\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db99d7c4-2e74-4300-911d-f6aad1f17a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54502ba-a18b-4b5c-b686-b51bbe671a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e769dac3-0efc-4cd4-955b-8fb652f5e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e97c72dc-8fbb-46b1-a5fb-257ef70dc514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-30.9709],\n",
       "        [-99.3821],\n",
       "        [  8.7239],\n",
       "        ...,\n",
       "        [-52.1168],\n",
       "        [-46.2518],\n",
       "        [ -4.3469]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664c5f1-de00-4c27-8c65-81a38ea728d3",
   "metadata": {},
   "source": [
    "It works!!! But there is plenty of room for improvement..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90026dff-2f2c-417f-9766-a30f440392c8",
   "metadata": {},
   "source": [
    "Luckily, Pytorch has an automatic differentiation engine, so we dont have to calcualte the gradients ourselves any more. Lets see how that would look like. We can start with restructuring our linear layer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6143384-9c30-45c0-ab29-60ebc3c24e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(in_features, out_features).requires_grad_()\n",
    "        self.b = torch.zeros(out_features).requires_grad_()\n",
    "    def forward(self, inp): \n",
    "        return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f93d1c3-51fe-4932-8c33-d37de8bd1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_layers, out_channels):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(in_channels,hidden_layers),\n",
    "                       nn.ReLU(),\n",
    "                       Linear(hidden_layers,out_channels)]\n",
    "        \n",
    "    def __call__(self, x, target):\n",
    "        for l in self.layers: \n",
    "            x = l(x)\n",
    "        return F.mse_loss(x, target.float()[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35861c36-3b32-4170-90dc-b3c14c2b034b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1549.3651, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, num_hidden, 1)\n",
    "loss = model(x_train, y_train.float())\n",
    "loss.backward()    \n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931e71f-785c-4094-aea6-8ff3909842d9",
   "metadata": {},
   "source": [
    "### Cross entropy loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7306c6-55ed-44a0-9163-5dc7b94479e6",
   "metadata": {},
   "source": [
    "In order to have a fully functioning model, we have to substitute our previous MSE loss with a better one. Cross entropy loss is a commonly used loss function for classification tasks. To implement it, we first have to compute the logarithm of the softmax and furthermore take the negative of the loss likelyhood. This can be achieved in these two simpe steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36a9734c-8a53-4354-9e42-9dc615899c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x): \n",
    "    return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "908b1b80-e9e4-4a78-894c-455504c17723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target): \n",
    "    return -input[range(target.shape[0]), target].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a87698c0-54a4-4fc2-9da0-da4bfd3420d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers: x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65416bb8-0b33-4432-8118-e8713cd2394b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(m, num_hidden, 10)\n",
    "pred = model(x_train)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81a9f302-3235-4b87-9f4a-11b29f935e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_pred = log_softmax(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa062adf-9a0a-494b-ba6e-08fe5172e52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3176, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(sm_pred, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64a57a-52a7-4dbe-9a86-9a345edda7dc",
   "metadata": {},
   "source": [
    "Great! Now that we have a better working loss function, we start thinking of an optimizer. \n",
    "The most common, and relatively simple to implement optimizer is Stochastic gradient descend.\n",
    "The two main functionalities, we have to implement are: \n",
    "* A step function \n",
    "* A function to zero out the gradients after each pass. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37657377-b964-455f-8319-268a296e292f",
   "metadata": {},
   "source": [
    "Lets see how that would look like in practice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1bca2be-ce49-4fb6-bd94-a396d31ed4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5): \n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for p in self.params: \n",
    "                p -= p.grad * self.lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.params: \n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9af718-6448-4a0c-acad-0e98988e4295",
   "metadata": {},
   "source": [
    "Some helper functions to help evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e31831b8-0143-4f1e-a947-ac1b839729c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb): \n",
    "    return (out.argmax(dim=1)==yb).float().mean()\n",
    "\n",
    "def report(loss, preds, yb): \n",
    "    print(f'Loss: {loss:.2f} | Accuracy: {accuracy(preds, yb):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "668b249a-b945-4fc3-a47b-54ef9f83052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,num_hidden), nn.ReLU(), nn.Linear(num_hidden,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53a46e55-8e41-470a-9364-556e5850573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89ce1607-18ec-404d-9706-290a9eecb8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 50 \n",
    "epochs = 3 \n",
    "xb,yb = x_train[:bs],y_train[:bs]\n",
    "preds = model(xb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95912523-308c-4924-8df5-353fb61e5e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.17 | Accuracy: 0.94\n",
      "Loss: 0.15 | Accuracy: 0.94\n",
      "Loss: 0.12 | Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for i in range(0, n, bs):\n",
    "        s = slice(i, min(n,i+bs))\n",
    "        xb,yb = x_train[s],y_train[s]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45341ce4-73ba-483e-acbd-a9469d8f9405",
   "metadata": {},
   "source": [
    "Great! Looks like our optimizer is working correctly and our model is training.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0e57c-a844-46c1-b3eb-0e36092fa94d",
   "metadata": {},
   "source": [
    "### The missing piece: Dataloaders and Dataset \n",
    "So far we have made solid progress, although oure code would be a lot cleaner, if we would implement a dataloader and dataset class\n",
    "Lets to that!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb5e257-9b4a-4c2e-b396-a87052862ee6",
   "metadata": {},
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0abcd806-7a60-4906-840d-363dc870be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, x, y): \n",
    "        self.x,self.y = x,y\n",
    "    def __len__(self): \n",
    "        return len(self.x)\n",
    "    def __getitem__(self, i): \n",
    "        return self.x[i],self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aa14366-d1f4-4144-b01d-75e2f8cd2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set,valid_data_set = Dataset(x_train, y_train),Dataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7b06b3f-b5a5-48d4-8f4f-b9986349ae24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 784]), tensor([5, 0, 4, 1, 9]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = train_data_set[0:5]\n",
    "xb.shape, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5239d65b-bc16-46cd-bb3d-d439e4873af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,num_hidden), nn.ReLU(), nn.Linear(num_hidden,10))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93b0ead9-eebf-4cd0-851c-ee57ea9932b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.35 | Accuracy: 0.92\n",
      "Loss: 0.26 | Accuracy: 0.92\n",
      "Loss: 0.21 | Accuracy: 0.92\n",
      "Loss: 0.17 | Accuracy: 0.92\n",
      "Loss: 0.14 | Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for i in range(0, n, bs):\n",
    "        xb,yb = train_data_set[i:min(n,i+bs)]\n",
    "        preds = model(xb)\n",
    "        loss = loss_func(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    report(loss, preds, yb)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf59ef0-765d-4bd6-91d5-8e6400f32540",
   "metadata": {},
   "source": [
    "#### Dataloader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8f2d782-ef6e-4d05-b2b9-33881a3a59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): \n",
    "        self.ds,self.bs = ds,bs\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.ds), self.bs): \n",
    "            yield self.ds[i:i+self.bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3d405202-2f14-4da5-884e-a3296543be59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl = DataLoader(train_data_set, bs)\n",
    "valid_dl = DataLoader(valid_data_set, bs)\n",
    "\n",
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ded04470-2f72-4182-afc2-fc71f51a5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m,num_hidden), nn.ReLU(), nn.Linear(num_hidden,10))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b43fec2a-5fa4-40cd-8621-8a06b5d0039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            # Do the forward pass\n",
    "            preds = model(xb)\n",
    "            # Calculate the loss\n",
    "            loss = loss_func(preds, yb)\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            # step the optimizer\n",
    "            optimizer.step()\n",
    "            \n",
    "        report(loss, preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c8acd09-5645-48e1-a7f8-1db4640e24a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.37 | Accuracy: 0.92\n",
      "Loss: 0.28 | Accuracy: 0.92\n",
      "Loss: 0.20 | Accuracy: 0.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.1767, grad_fn=<NllLossBackward0>), tensor(0.9800))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit()\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab365f-f98e-4fcf-9366-b3d01182aa0a",
   "metadata": {},
   "source": [
    "Perfect! Our model is training correctly once again. This leaves us with one final step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75957adc-df23-456c-afb9-e7823818d89d",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee719921-0269-4828-a63c-420a1f51b13f",
   "metadata": {},
   "source": [
    "The last step is to include our validation set. The validation set is very important because it tells us if we are overfitting on the training data. We can now substitute our manual dataloaders with PyTorches DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87c9192a-6396-484c-9803-770344df2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dl = DataLoader(train_data_set, bs, shuffle=True, drop_last=True, num_workers=2)\n",
    "valid_dl = DataLoader(valid_data_set, bs, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27521385-fdf2-43f6-af40-43676e92f23a",
   "metadata": {},
   "source": [
    "Great! Now it is time for the training loop with the addition of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b139b9a-b6cd-4496-99ed-21492428e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            # Calculate the loss\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            \n",
    "            # Zero out the gradients\n",
    "            opt.zero_grad()\n",
    "            # Do the backward pass\n",
    "            loss.backward()\n",
    "            # Step the optimizer\n",
    "            opt.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss, test_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                test_loss += loss_func(pred,yb).item()*n\n",
    "                test_acc  += accuracy (pred,yb).item()*n\n",
    "\n",
    "        print(f\"epoch: {epoch} | test loss: {test_loss/count} | Test accuracy: {test_acc/count}\")\n",
    "        \n",
    "    return test_loss/count, test_acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75b10077-c32d-47e1-9ebb-17b4ee4b7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "067c3c24-d110-4450-9f55-3b04dad291f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | test loss: 0.3048352462425828 | Test accuracy: 0.915900000333786\n",
      "epoch: 1 | test loss: 0.25316496428102253 | Test accuracy: 0.9290000027418137\n",
      "epoch: 2 | test loss: 0.21493432955816388 | Test accuracy: 0.9406000018119812\n",
      "epoch: 3 | test loss: 0.1852528131380677 | Test accuracy: 0.9477000004053115\n",
      "epoch: 4 | test loss: 0.1661680996976793 | Test accuracy: 0.9544000053405761\n",
      "epoch: 5 | test loss: 0.15068742735311388 | Test accuracy: 0.9596000051498413\n",
      "epoch: 6 | test loss: 0.14419448643922805 | Test accuracy: 0.9585000038146972\n",
      "epoch: 7 | test loss: 0.13473034169524908 | Test accuracy: 0.9620000052452088\n",
      "epoch: 8 | test loss: 0.1290793955512345 | Test accuracy: 0.9632000041007995\n",
      "epoch: 9 | test loss: 0.11868104227818549 | Test accuracy: 0.9660000044107437\n",
      "epoch: 10 | test loss: 0.1194089087517932 | Test accuracy: 0.9661000055074692\n",
      "epoch: 11 | test loss: 0.11515257748775184 | Test accuracy: 0.9650000089406967\n",
      "epoch: 12 | test loss: 0.10903757377061993 | Test accuracy: 0.96760000705719\n",
      "epoch: 13 | test loss: 0.10710082604084165 | Test accuracy: 0.9679000049829483\n",
      "epoch: 14 | test loss: 0.10193296607583761 | Test accuracy: 0.9695000052452087\n",
      "epoch: 15 | test loss: 0.1014062008075416 | Test accuracy: 0.9708000046014785\n",
      "epoch: 16 | test loss: 0.09831526305526495 | Test accuracy: 0.970400007367134\n",
      "epoch: 17 | test loss: 0.09669984467327594 | Test accuracy: 0.9713000071048736\n",
      "epoch: 18 | test loss: 0.09910286450758576 | Test accuracy: 0.9696000051498413\n",
      "epoch: 19 | test loss: 0.10073822749778628 | Test accuracy: 0.9721000069379806\n",
      "CPU times: user 5.87 s, sys: 898 ms, total: 6.77 s\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data_loader, valid_data_loader = get_data_loaders(train_data_set, valid_data_set, bs)\n",
    "model = nn.Sequential(nn.Linear(m,num_hidden), nn.ReLU(), nn.Linear(num_hidden,10))\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.05)   \n",
    "\n",
    "%time loss,acc = fit(20, model, loss_func, opt, train_data_loader, valid_data_loader)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f49b58c-dd3d-4f7b-acdd-62acbfb98690",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af28164-a93c-47e8-bbc5-f5e605f6d85d",
   "metadata": {},
   "source": [
    "This project has provided a comprehensive exploration into the implementation of a neural network from scratch. By diving into the foundational components such as linear layers, activation functions, and loss functions, I have gained a deep understanding of the inner mechanics in a neural network.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
